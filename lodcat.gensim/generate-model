#!/usr/bin/env python3
from dicetopicmodelingxmlcorpus import DiceTopicModelingXmlCorpus
from gensim.corpora import MmCorpus
from gensim.corpora.dictionary import Dictionary
from gensim.models import LdaModel
from itertools import chain
import argparse
import csv
import logging
import os

corpus_types = {
    'xml': DiceTopicModelingXmlCorpus,
    'mm': MmCorpus,
}

MODEL_FILE = 'gensim-ldamodel'
TOP_WORDS_FILE = 'top_words.csv'

logging.basicConfig(level=logging.DEBUG)

parser = argparse.ArgumentParser()
parser.add_argument('--input', dest='input_file', required=True, help='corpus file')
parser.add_argument('--input-format', dest='input_format', required=True, choices=corpus_types.keys(), help='corpus type')
parser.add_argument('--output', dest='output_dir', required=True, help='output directory')
parser.add_argument('--dictionary', dest='dictionary_file', required=True, help='gensim dictionary file')
parser.add_argument('--num-topics', dest='num_topics', required=True, type=int, help='number of topics')
args = parser.parse_args()

dictionary = Dictionary.load(args.dictionary_file)  # if args.dictionary_file else None
logging.info('Dictionary: %s', dictionary)

CorpusClass = corpus_types[args.input_format]
if CorpusClass is DiceTopicModelingXmlCorpus:
    corpus = CorpusClass(args.input_file, dictionary=dictionary)
else:
    corpus = CorpusClass(args.input_file)

os.makedirs(args.output_dir, exist_ok=True)
model_file = os.path.join(args.output_dir, MODEL_FILE)
top_words_file = os.path.join(args.output_dir, TOP_WORDS_FILE)

num_topics = args.num_topics
chunksize = 100_000
iterations = 1000
eval_every = 1000

model = LdaModel(
    corpus=corpus,
    id2word=dictionary.id2token,
    chunksize=chunksize,
    alpha='auto',
    eta='auto',
    iterations=iterations,
    num_topics=num_topics,
    passes=1,  # one pass through the corpus
    update_every=0,  # batch learning
    eval_every=eval_every
)

logging.info('Model: %s', model_file)
model.save(model_file)

# Generate top_words.csv
topn = 10
top_words = [model.get_topic_terms(topic_id, topn=topn) for topic_id in range(0, num_topics)]
logging.info('Top words: %s', top_words_file)
with open(top_words_file, 'w') as csvfile:
    csvwriter = csv.writer(csvfile, delimiter=';', lineterminator='\n')
    csvwriter.writerow(chain.from_iterable((f'topic{topic_id}', '') for topic_id in range(0, num_topics)))
    for n in range(0, topn):
        csvwriter.writerow(chain.from_iterable((dictionary.id2token.get(word_id), prob) for word_id, prob in (top_words[topic_id][n] for topic_id in range(0, num_topics))))
